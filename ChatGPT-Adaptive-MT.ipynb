{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dnzq6rCBj8s3iRHRvUCK121ffef6jvJw",
      "authorship_tag": "ABX9TyNHwL5IYuVsZNX9ms9OJVuq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ymoslem/Adaptive-MT-LLM-Fine-tuning/blob/main/ChatGPT-Adaptive-MT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Translation with ChatGPT\n",
        "\n",
        "This notebook is part of the repository [Adaptive-MT-LLM-Fine-tuning](https://github.com/ymoslem/Adaptive-MT-LLM-Fine-tuning)."
      ],
      "metadata": {
        "id": "ta3i3wddYId3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load files"
      ],
      "metadata": {
        "id": "2yFOBmnFLUEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/data/\"\n",
        "directory = os.path.join(data_path, \"spanish\")\n",
        "\n",
        "os.chdir(directory)\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "thux1ESjCF9H",
        "outputId": "859152a6-db31-4feb-b2d4-0048a1a7fe67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/data/spanish'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load test dataset\n",
        "\n",
        "source_test_file = \"all-filtered.es.real.test\"\n",
        "target_test_file = \"all-filtered.en.real.test\"\n",
        "\n",
        "with open(source_test_file, encoding=\"utf-8\") as source, open(target_test_file, encoding=\"utf-8\") as target:\n",
        "  source_sentences = [sent.strip() for sent in source.readlines()]\n",
        "  target_sentences = [sent.strip() for sent in target.readlines()]\n",
        "\n",
        "print(source_sentences[0])\n",
        "print(target_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdQA6lbBCI5M",
        "outputId": "cbd2fbab-b90d-42c9-def7-8e8742174e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Período de validez después de abierto el envase: 10 horas.\n",
            "Shelf life after first opening the container: 10 hours.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load fuzzy matches from the Context Dataset\n",
        "\n",
        "online_test_file = \"all-filtered.esen.ms-multi-12.online.test\"\n",
        "\n",
        "with open(online_test_file, encoding=\"utf-8\") as online:\n",
        "  lines = [line.strip().split(\" ||| \") for line in online.readlines()]\n",
        "  scores = [float(line[0].strip()) for line in lines]\n",
        "  fuzzy_source_sentences = [line[1].strip() for line in lines]\n",
        "  online_source_sentences = [line[2].strip() for line in lines]\n",
        "  fuzzy_target_prefixes = [line[3].strip() for line in lines]\n",
        "\n",
        "print(fuzzy_source_sentences[0])\n",
        "print(online_source_sentences[0])\n",
        "print(fuzzy_target_prefixes[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMB8lQLCCQ2g",
        "outputId": "5435b351-35c8-44b5-faa3-fa3aa59a132f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Período de validez después de abierto el envase: 4 semanas\n",
            "Período de validez después de abierto el envase: 10 horas.\n",
            "Shelf life after opening the immediate packaging: 4 weeks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create prompts"
      ],
      "metadata": {
        "id": "DYb9dxsAQtpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create zero-shot and one-shot prompts\n",
        "\n",
        "def create_prompt(source_lang,\n",
        "                  target_lang,\n",
        "                  fuzzy_sources,\n",
        "                  fuzzy_targets,\n",
        "                  new_sources,\n",
        "                  one_shot=True\n",
        "                  ):\n",
        "\n",
        "  prompts = []\n",
        "\n",
        "  if one_shot:\n",
        "    for fuzzy_src, fuzzy_tgt, new_src in zip(fuzzy_sources, fuzzy_targets, new_sources):\n",
        "      fuzzy_src = source_lang + \": \" + fuzzy_src\n",
        "      fuzzy_tgt = target_lang + \": \" + fuzzy_tgt\n",
        "      new_src = source_lang + \": \" + new_src\n",
        "      segment = fuzzy_src + \"\\n\" + fuzzy_tgt + \"\\n\" + new_src + \"\\n\" + target_lang + \":\"\n",
        "      prompts.append(segment)\n",
        "  else:\n",
        "    for new_src in new_sources:\n",
        "      new_src = source_lang + \": \" + new_src\n",
        "      segment = new_src + \"\\n\" + target_lang + \":\"\n",
        "      prompts.append(segment)\n",
        "\n",
        "  return prompts"
      ],
      "metadata": {
        "id": "lhtPPU5zDFp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_lang = \"Spanish\"\n",
        "target_lang = \"English\""
      ],
      "metadata": {
        "id": "l_XXEc-cQvno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create prompts\n",
        "# Set one_shot=True to create a one-shot prompts\n",
        "\n",
        "prompts = create_prompt(source_lang,\n",
        "                        target_lang,\n",
        "                        fuzzy_source_sentences,\n",
        "                        fuzzy_target_prefixes,\n",
        "                        online_source_sentences,\n",
        "                        one_shot=False\n",
        "                        )\n",
        "\n",
        "print(len(prompts))"
      ],
      "metadata": {
        "id": "FVTTpMt8QxKX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27cbd567-1158-491f-e4c8-219be9e3e4ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompts[0], \"\\n\")\n",
        "print(prompts[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAlrExMiEDhk",
        "outputId": "95e17dc4-4585-48a8-b307-404f7840ae15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spanish: Período de validez después de abierto el envase: 10 horas.\n",
            "English: \n",
            "\n",
            "Spanish: El mecanismo implicado en esta posible asociación es aún especulativo pero puede reflejar la mayor frecuencia en mujeres por la disfunción del esfínter de Oddi como lo señalado por Freeman y cols en su estudio 2.\n",
            "English:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generation"
      ],
      "metadata": {
        "id": "FXV2gNR-LWu1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HeVV8GubKcHV"
      },
      "outputs": [],
      "source": [
        "!pip3 install openai --upgrade -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get OpenAI API key from Colab Secrets\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get(\"openai_api_key\")"
      ],
      "metadata": {
        "id": "o_216BV0q3mB"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatGPT generation function\n",
        "# model: You can change \"gpt-3.5-turbo\" to \"gpt-4\", but for higher costs!\n",
        "\n",
        "import openai\n",
        "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
        "\n",
        "\n",
        "# ✳️ Add your OpenAI API key\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=2, max=60), stop=stop_after_attempt(6))\n",
        "def translate(prompt, max_tokens, model, temperature=0.3, top_p=1):\n",
        "  response = openai.chat.completions.create(\n",
        "                                          model=model,\n",
        "                                          temperature=temperature,\n",
        "                                          max_tokens=max_tokens,\n",
        "                                          messages=[\n",
        "                                          {\"role\": \"user\",\n",
        "                                          \"content\": prompt}\n",
        "                                          ],\n",
        "                                          top_p=top_p,\n",
        "                                          frequency_penalty=0,\n",
        "                                          presence_penalty=0,\n",
        "                                          n=1,\n",
        "                                          #stop=[\"\\n\"],\n",
        "  )\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "5nm0YbcxLIB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "test_translation = translate(prompt=prompts[0], max_tokens=100, model=\"gpt-3.5-turbo-1106\")\n",
        "print(test_translation.choices[0].message.content.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgr8djzsL6iO",
        "outputId": "fc7fe039-1c86-4273-8efa-4262e8ce3f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shelf life after opening the package: 10 hours.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch Processing"
      ],
      "metadata": {
        "id": "icn7U4ODL1Pw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sending batch requsets\n",
        "\n",
        "from concurrent import futures\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "num_workers = 128\n",
        "\n",
        "def batch_translate(prompts, **kwargs):\n",
        "  with futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    response = executor.map(lambda prompt: translate(prompt=prompt, **kwargs), prompts)\n",
        "  return list(response)"
      ],
      "metadata": {
        "id": "iIXH8GVHLb7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Devide a long list of source sentences into smaller batches\n",
        "\n",
        "def divide_chunks(l, n):\n",
        "  # looping till length l\n",
        "  for i in range(0, len(l), n):\n",
        "    yield l[i:i + n]"
      ],
      "metadata": {
        "id": "BM-0YkZuR0ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "\n",
        "temperature = 0.3\n",
        "top_p = 1\n",
        "\n",
        "# ✳️ Change the batch size for longer inputs/outputs\n",
        "# Note: Trial accounts allow only 3 requests per minute\n",
        "batch_size = 20\n",
        "\n",
        "# ✳️ Change number of source words vs target tokens.\n",
        "# Try 4 for French and Spanish; it can be 5 for some other languages like Arabic.\n",
        "# You can also use the \"tiktoken\" library to tokenize the source,\n",
        "# and then length_multiplier can be based on tokens rather than words.\n",
        "length_multiplier = 4"
      ],
      "metadata": {
        "id": "qjpu8ANWfFIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model name\n",
        "\n",
        "model = \"gpt-3.5-turbo\"\n",
        "\n",
        "# Other models\n",
        "# model = \"gpt-3.5-turbo-1106\"\n",
        "# model = \"gpt-4\"\n",
        "# model = \"gpt-4-1106-preview\"  # GPT-4 TurboNew"
      ],
      "metadata": {
        "id": "q0WoAPhJo51U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch translation\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "import json\n",
        "\n",
        "start = 2380 # change to 0\n",
        "\n",
        "# Translate\n",
        "translations = []\n",
        "total = int(len(prompts[start:])/batch_size)\n",
        "\n",
        "\n",
        "with open(\"temp_output.json\", \"a\") as output_file:\n",
        "\n",
        "  for chunk_prompts in tqdm(divide_chunks(prompts[start:], batch_size), total=total):\n",
        "    length = [len(prompt.split(\"\\n\")[-2].split(\" \")[1:]) for prompt in chunk_prompts]\n",
        "    max_len = max(length) * length_multiplier\n",
        "\n",
        "    outputs = batch_translate(prompts = chunk_prompts,\n",
        "                              max_tokens = max_len,\n",
        "                              model = model,\n",
        "                              temperature=temperature,\n",
        "                              top_p = top_p)\n",
        "    batch_translations = [output.choices[0].message.content.strip() for output in outputs]\n",
        "    translations += batch_translations\n",
        "\n",
        "    output_translations = [{\"translation\": translation.strip()} for translation in batch_translations]\n",
        "    output_translations = \"\\n\".join([json.dumps(translation, ensure_ascii=False) for translation in output_translations])\n",
        "    # Write raw translations to a JSON file (without handling over-generation)\n",
        "    output_file.write(output_translations + \"\\n\")\n",
        "    output_file.flush()\n",
        "\n",
        "    sleep(10)\n",
        "\n",
        "\n",
        "# Report stats\n",
        "print(\"Translations:\", len(translations), end=\"\\n\\n\")\n",
        "print(\"• Last Translation:\")\n",
        "print(\"Prompt Tokens:\", outputs[-1].usage.prompt_tokens)\n",
        "print(\"Completion Tokens:\", outputs[-1].usage.completion_tokens)\n",
        "print(\"Total Tokens:\", outputs[-1].usage.total_tokens, end=\"\\n\\n\")\n",
        "print(prompts[-1], end=\" \")\n",
        "print(translations[-1], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "Y2tbPOetSByU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(translations)"
      ],
      "metadata": {
        "id": "h_gfH2BWOxi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 5 translations\n",
        "print(*translations[:5], sep=\"\\n\")"
      ],
      "metadata": {
        "id": "rg4K3jvGd7SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save translations"
      ],
      "metadata": {
        "id": "R3pP-eLLpo4a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translations_file_name = \"all-filtered.esen.ms-multi-12.online.test.translated-ChatGPT-gpt-3.5-turbo-zero-shot.en\"\n",
        "# translations_file_name = \"all-filtered.esen.ms-multi-12.online.test.translated-ChatGPT-gpt-3.5-turbo-one-shot.en\""
      ],
      "metadata": {
        "id": "BgHChX4sdY0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install nltk -q\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "id": "YBOYyUxh9A2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save translations to a file\n",
        "# This code also handles over-generation\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "import os\n",
        "\n",
        "# ✳️ Where to save the translations\n",
        "# It is better to connect Google Drive, and change 'directory'\n",
        "directory = \"\"\n",
        "output_file_name = translations_file_name\n",
        "output_path = os.path.join(directory, output_file_name)\n",
        "\n",
        "with open(output_path, \"w+\") as translated_file:\n",
        "  for source, translation in zip(source_sentences, translations):\n",
        "    translation = translation.strip()\n",
        "    if \"\\n\" in translation:\n",
        "      translation = translation.split(\"\\n\")[0]\n",
        "      translated_file.write(translation.strip() + \"\\n\")\n",
        "    elif len(sent_tokenize(translation)) > len(sent_tokenize(source)) and len(word_tokenize(translation)) > len(word_tokenize(source))*2:\n",
        "      translation = sent_tokenize(translation)[0]\n",
        "      translated_file.write(translation.strip() + \"\\n\")\n",
        "    else:\n",
        "      translated_file.write(translation.strip() + \"\\n\")\n",
        "\n",
        "print(\"Translation file saved at:\", output_path)"
      ],
      "metadata": {
        "id": "MwQk9YgIpn4v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}